---
title: "Trend Data to Elmer"
output: html_notebook
---
This code downloads and compiles various trends data for incorporation into standard datatables in the Central Database.
```{r setup, include=FALSE}

library(tidyverse)
library(readxl)
library(data.table)
library(lubridate)
library(odbc)
library(DBI)

```

This chunk of code pulls down the annual Office of Financial Management (OFM) blockgroup population and housing data between 2000 and the latest data year and stores it in a long-form data.table for inclusion into the Central Database. The data is made available in the fall of every year (by the end of September).

```{r ofm_population_data, include=FALSE}
ofm_url <- 'https://www.ofm.wa.gov/sites/default/files/public/legacy/pop/smallarea/data/xlsx/saep_bg10.xlsx'

# Details from OFM file including a list of tabs in the spreadsheet to iterate over
download.file(ofm_url, "working.xlsx", quiet = FALSE, mode = "wb")
ofm_pop_file <- paste0(getwd(),"/working.xlsx")

list_of_tabs <- excel_sheets(ofm_pop_file)
ofm_dt <- NULL

# Create a long form data.table of all OFM variables for inlcusion in Central DB
for (tabs in list_of_tabs) {
  ofm_pop_bg <- read_excel(ofm_pop_file,sheet = tabs, skip = 10)
  setDT(ofm_pop_bg)
  ofm_pop_bg <- na.omit(ofm_pop_bg)

  # Remove the columns that have deltas calculated
  cols_delta <- "([2][0-9]{3} to [2][0-9]{3})"
  cols_to_remove <- str_subset(colnames(ofm_pop_bg), cols_delta)
  ofm_pop_bg <- ofm_pop_bg[, (cols_to_remove):=NULL]

  # Drop Columns that are not the BlockGroup or Yearly Results
  cols_year <- "([2][0-9]{3})"
  cols_to_keep <- str_subset(colnames(ofm_pop_bg), cols_year)
  cols_to_keep <- c("Census Block Group Code Complete",cols_to_keep)
  ofm_pop_bg <- ofm_pop_bg[, ..cols_to_keep]

  # Now Rename columns to GEOID and Year before the data is transformed to long form
  cols_year <- "([2][0-9]{3})"
  updated_names <- str_subset(colnames(ofm_pop_bg), cols_year)
  updated_names <- str_sub(updated_names, -4, -1)
  updated_names <- c("geoid",updated_names)
  setnames(ofm_pop_bg,updated_names)

  working_dt <- melt(ofm_pop_bg, id.vars = c("geoid"), measure.vars = str_subset(colnames(ofm_pop_bg), cols_year))
  working_dt$date <- paste0("04/01/", working_dt$variable)
  working_dt <- working_dt[, date:=as.Date(date,"%d/%m/%Y")]
  working_dt$attribute <- tabs
  working_dt <- working_dt[, variable:=NULL]

  ifelse(is.null(ofm_dt), ofm_dt <- working_dt, ofm_dt <- rbindlist(list(ofm_dt, working_dt)))
}

ofm_dt <- ofm_dt[, record_id := .I]
final_column_order <- c('record_id','geoid','date','attribute','value')
setcolorder(ofm_dt, final_column_order)

# Now delete the temporary downloaded file
file.remove(ofm_pop_file)

```

This chunk of code pulls down the monthly Employment Security Division (ESD) metropolitan statistical area covered employment data between 1990 and the latest monthly data and stores it in a long-form data.table for inclusion into the Central Database. The data is made available in the last week of each month.

```{r esd_employment_data, include=FALSE}
esd_url <- 'https://esdorchardstorage.blob.core.windows.net/esdwa/Default/ESDWAGOV/labor-market-info/Libraries/Economic-reports/Washington-employment-estimates/WA-QB-historical-SA.xlsx'

# Details from ESD file including a list of tabs in the spreadsheet to iterate over
download.file(esd_url, "working.xlsx", quiet = FALSE, mode = "wb")
esd_job_file <- paste0(getwd(),"/working.xlsx")

list_of_tabs <- excel_sheets(esd_job_file)
list_of_tabs <- list_of_tabs[!list_of_tabs %in% "Index"]
esd_dt <- NULL

# Create a long form data.table of all OFM variables for inlcusion in Central DB
for (tabs in list_of_tabs) {
  esd_job_msa <- read_excel(esd_job_file,sheet = tabs, skip = 1)
  setDT(esd_job_msa)
  esd_job_msa <- na.omit(esd_job_msa)

  # Get list of the monthly columns
  month_pattern <- "([0-9]{5})"
  monthly_columns <- str_subset(colnames(esd_job_msa), month_pattern)
  monthly_columns <- as.integer(monthly_columns)
  monthly_columns <- monthly_columns - 25569
  updated_names <- c("naics_code","naics_name",as.character(as_date(monthly_columns)))
  setnames(esd_job_msa,updated_names)
  
  working_dt <- melt(esd_job_msa, id.vars = c("naics_code","naics_name"), measure.vars = as.character(as_date(monthly_columns)))
  
  working_dt$date <- as_date(working_dt$variable)
  working_dt$msa_name <- tabs
  working_dt <- working_dt[, variable:=NULL]

  ifelse(is.null(esd_dt), esd_dt <- working_dt, esd_dt <- rbindlist(list(esd_dt, working_dt)))
}

esd_dt <- esd_dt[, record_id := .I]
final_column_order <- c('record_id','naics_code','naics_name','date','msa_name','value')
setcolorder(esd_dt, final_column_order)

# Now delete the temporary downloaded file
file.remove(esd_job_file)

```

This chunk of code pulls down the monthly National Transit Database data between 2002 and the latest monthly data and stores it in a long-form data.table for inclusion into the Central Database. The data is made available in the second week of each month.

```{r ntd_data, include=FALSE}
current_day <- day(Sys.Date())

if (current_day <= 15) {
  data_month <- months(as.Date(Sys.Date()) %m-% months(3))
  data_year <- as.integer(year(as.Date(Sys.Date()) %m-% months(3)))

} else {
  data_month <- months(as.Date(Sys.Date()) %m-% months(2))
  data_year <- as.integer(year(as.Date(Sys.Date()) %m-% months(2)))
}

working_url <- paste0('https://www.transit.dot.gov/sites/fta.dot.gov/files/',as.character(data_month),'%20',as.character(data_year),'%20Adjusted%20Database.xlsx')

# Details from downloaded file including a list of tabs in the spreadsheet to iterate over
download.file(working_url, "working.xlsx", quiet = FALSE, mode = "wb")
working_file <- paste0(getwd(),"/working.xlsx")

list_of_tabs <- excel_sheets(working_file)
list_of_tabs <- list_of_tabs[list_of_tabs %in% c("UPT","VRM","VRH")]
ntd_dt <- NULL

# Create a long form data.table of all variables for inclusion in Central DB
for (tabs in list_of_tabs) {
  working_dt <- read_excel(working_file,sheet = tabs, skip = 0)
  setDT(working_dt)

  # Remove inactive reporters and non-reporters from the trend data and clean columns
  working_dt <- working_dt[Active %in% c("Active")]
  working_dt <- working_dt[`Reporter Type` %in% c("Full Reporter")]
  working_dt <- working_dt[, `4 digit NTD ID`:=NULL]
  working_dt <- working_dt[, Active:=NULL]
  working_dt <- working_dt[, `Reporter Type`:=NULL]

  # Get list of the monthly columns that are the measure variables in flattened data.table
  month_pattern <- "([0-9]{2})"
  monthly_columns <- str_subset(colnames(working_dt), month_pattern)
  updated_colnames <- c("ntd_id","agency_name","uza_code","uza_name","modes","tos",monthly_columns)
  setnames(working_dt,updated_colnames)
  
  variable_columns <- c("ntd_id","agency_name","uza_code","uza_name","modes","tos")
  working_dt <- melt(working_dt, id.vars = variable_columns, measure.vars = monthly_columns)
  
  working_dt$month <- substr(working_dt$variable, 1, 3)
  working_dt$year <- substr(working_dt$variable, 4, 5)
  working_dt$date <- paste0(working_dt$month,"/01/20", working_dt$year)
    
  # Convert the variable column to a date
  working_dt$date <- mdy(working_dt$date)
  working_dt$attribute <- tabs
  working_dt <- working_dt[, variable:=NULL]
  working_dt <- working_dt[, month:=NULL]
  working_dt <- working_dt[, year:=NULL]

  ifelse(is.null(ntd_dt), ntd_dt <- working_dt, ntd_dt <- rbindlist(list(ntd_dt, working_dt)))
}

ntd_dt <- ntd_dt[, record_id := .I]
final_column_order <- c('record_id','ntd_id','agency_name','uza_code','uza_name','modes','tos','date','attribute','value')
setcolorder(ntd_dt, final_column_order)

# Now delete the temporary downloaded file
file.remove(working_file)

```

This chunk of code pulls down the quarterly Bureau of Labor Statistics between 2014 and the latest quarterly data and stores it in a long-form data.table for inclusion into the Central Database. The data is made available quarterly.

```{r bls_data, include=FALSE}

analysis_years <- c(2014,2015,2016,2017,2018)
analysis_quarters <- c(1,2,3,4)

compile_bls_data <- function(working_qtr, working_year, working_dt) {
  working_url <- paste0('http://www.bls.gov/cew/data/api/', as.character(working_year), '/', as.character(working_qtr), '/industry/10.csv')
  
  download.file(working_url, "working.csv", quiet = FALSE, mode = "wb")
  working_file <- paste0(getwd(),"/working.csv")
  current_dt <- read.csv(working_file)
  setDT(current_dt)
  columns_to_keep <- c("area_fips","own_code","industry_code","year","qtr","month1_emplvl","month2_emplvl","month3_emplvl","avg_wkly_wage")

  current_dt <- current_dt[, ..columns_to_keep]
  current_dt <- current_dt[own_code== 0]
  current_dt <- current_dt[industry_code== 10]  
  ifelse(is.null(working_dt), working_dt <- current_dt, working_dt <- rbindlist(list(working_dt, current_dt)))
    
  return(working_dt)
}

# Loop over downloads from BLS and create a master data.table
working_dt <- NULL
for (current_year in analysis_years) {
  for (current_qtr in analysis_quarters) {
    try(working_dt <- compile_bls_data(current_qtr,current_year, working_dt))
  }

}

# Remove a couple columns thjat are not really needed.
working_dt <- working_dt[, own_code:=NULL]
working_dt <- working_dt[, industry_code:=NULL]
updated_names <- c("area_fips","year","qtr","month1","month2","month3","weekly_wages")
setnames(working_dt,updated_names)

variable_columns <- c("area_fips","year","qtr")
measure_columns <- c("month1","month2","month3","weekly_wages")
bls_dt  <- melt(working_dt , id.vars = variable_columns, measure.vars = measure_columns)
bls_dt$variable <- as.character(bls_dt$variable)
bls_dt$area_fips <- as.character(bls_dt$area_fips)

# Need to clean this but for now it works
bls_dt[(variable == "month1" & qtr == 1), month := "jan"]
bls_dt[(variable == "month2" & qtr == 1), month := "feb"]
bls_dt[(variable == "month3" & qtr == 1), month := "mar"]
bls_dt[(variable == "month1" & qtr == 2), month := "apr"]
bls_dt[(variable == "month2" & qtr == 2), month := "may"]
bls_dt[(variable == "month3" & qtr == 2), month := "jun"]
bls_dt[(variable == "month1" & qtr == 3), month := "jul"]
bls_dt[(variable == "month2" & qtr == 3), month := "aug"]
bls_dt[(variable == "month3" & qtr == 3), month := "sep"]
bls_dt[(variable == "month1" & qtr == 4), month := "oct"]
bls_dt[(variable == "month2" & qtr == 4), month := "nov"]
bls_dt[(variable == "month3" & qtr == 4), month := "dec"]
bls_dt[(variable == "weekly_wages" & qtr == 1), month := "jan"]
bls_dt[(variable == "weekly_wages" & qtr == 2), month := "apr"]
bls_dt[(variable == "weekly_wages" & qtr == 3), month := "jul"]
bls_dt[(variable == "weekly_wages" & qtr == 4), month := "oct"]

bls_dt$date <- paste0(bls_dt$month,"/01/", bls_dt$year)
bls_dt$date <- mdy(bls_dt$date)
bls_dt <- bls_dt[, year:=NULL]
bls_dt <- bls_dt[, qtr:=NULL]
bls_dt <- bls_dt[, month:=NULL]

bls_dt[(variable == "month1" | variable == "month2" | variable == "month3"), variable := "jobs"]

bls_dt <- bls_dt[, record_id := .I]
final_column_order <- c('record_id','area_fips','variable','date','value')
setcolorder(bls_dt, final_column_order)

# Now delete the temporary downloaded file
file.remove(working_file)

```

This chunk of code pulls down the monthly FHWA VMT data between 2010 and the latest monthly data and stores it in a long-form data.table for inclusion into the Central Database. The data is made available monthly.

```{r fhwa_vmt_data, include=FALSE}

analysis_years <- c("12","13","14","15","16","17","18")
analysis_months <- c("jan","feb","mar","apr","may","jun","jul","aug","sep","oct","nov","dec")
list_of_tabs <- c("Page 4","Page 5","Page 6")

download_fhwa_vmt <- function(working_year, working_month){
    tryCatch(
        {
        working_url <- paste0('https://www.fhwa.dot.gov/policyinformation/travel_monitoring/', working_year, working_month,'tvt/', working_year, working_month,'tvt.xls')
        download.file(working_url, "working.xls", quiet = FALSE, mode = "wb")
        working_file <- paste0(getwd(),"/working.xls")
        return(working_file)
        },
        
        error=function(error_message) {
          
          # First try and see if it is an xlsx file otherwise just move on
          tryCatch(
            {
            working_url <- paste0('https://www.fhwa.dot.gov/policyinformation/travel_monitoring/', working_year, working_month,'tvt/', working_year, working_month,'tvt.xlsx')
            download.file(working_url, "working.xlsx", quiet = FALSE, mode = "wb")
            working_file <- paste0(getwd(),"/working.xlsx")
            return(working_file)
            },
            
            error=function(error_message) {
              message(error_message)
            }
          )
        }
    )
} 

# Loop over downloads from BLS and create a master data.table
fhwa_vmt_dt <- NULL
for (working_year in analysis_years) {
  
  for (working_month in analysis_months) {
    
    working_file <-download_fhwa_vmt(working_year,working_month)
    
    if (!is.null(working_file)) {
  
      for (tabs in list_of_tabs) {
  
        current_month <- mdy(paste0(working_month,"-01-20",working_year))
        previous_month <- current_month %m-% months(1)

        current_dt <- read_excel(working_file,sheet = tabs, skip = 4)
        setDT(current_dt)
  
        data_columns <- c("..1", str_subset(colnames(current_dt), "Revised"))
        current_dt <- current_dt[, ..data_columns]
        updated_names <- c("state",as.character(previous_month))
        setnames(current_dt, updated_names)
        current_dt <- na.omit(current_dt)
        current_dt <- current_dt[!(state %in% c("Subtotal","TOTALS","District of Columbia"))]
        convert_to_num <- c(as.character(previous_month))
        current_dt[, convert_to_num] <- current_dt[, lapply(.SD, as.numeric), .SDcols = convert_to_num] *1000000
  
        variable_columns <- c("state")
        measure_columns <- c(as.character(previous_month))
        current_dt  <- melt(current_dt , id.vars = variable_columns, measure.vars = measure_columns)
        current_dt$date <- ymd(current_dt$variable)
        current_dt <- current_dt[, variable:=NULL]
  
        ifelse(tabs == "Page 4", current_dt$attribute <- "rural arterials", ifelse(tabs == "Page 5", current_dt$attribute <- "urban arterials", current_dt$attribute <- "all roadways"))
  
        ifelse(is.null(fhwa_vmt_dt), fhwa_vmt_dt <- current_dt, fhwa_vmt_dt <- rbindlist(list(fhwa_vmt_dt, current_dt)))
      }
    }
  } 
}  
      
fhwa_vmt_dt <- fhwa_vmt_dt[, record_id := .I]
final_column_order <- c('record_id','state','attribute','date','value')
setcolorder(fhwa_vmt_dt, final_column_order)

# Now delete the temporary downloaded file
file.remove(working_file)

```

This chunk of code stores the various data.tables from the previous code chunks and puts them in the Central Database.

```{r write_to_db, include = FALSE}

# SQL Database Connection settings
elmer_connection <- dbConnect(odbc::odbc(),
  driver = "SQL Server",
  server = "sql2016\\DSADEV",
  database = "Sandbox",
  trusted_connection = "yes"
  )

# Write data to Central Database
dbWriteTable(elmer_connection, "ofm_blockgroup_pop_housing", ofm_dt, overwrite=TRUE)
dbWriteTable(elmer_connection, "esd_wa_monthly_msa_jobs", esd_dt, overwrite=TRUE)
dbWriteTable(elmer_connection, "ntd_monthly_transit_data", ntd_dt, overwrite=TRUE)
dbWriteTable(elmer_connection, "bls_national_wage_job_data", bls_dt, overwrite=TRUE)
dbWriteTable(elmer_connection, "fhwa_monthly_statewide_vmt", fhwa_vmt_dt, overwrite=TRUE)

# Close Connections to Central Databas
dbDisconnect(elmer_connection)

```
